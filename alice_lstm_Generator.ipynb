{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1\n"
      ],
      "metadata": {
        "id": "pnOANkWTXpOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL for Alice's Adventures in Wonderland from Project Gutenberg\n",
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "text_file_name = \"alice_in_wonderland.txt\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status() # Raise an exception for HTTP errors\n",
        "    with open(text_file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "    print(f\"'{text_file_name}' downloaded successfully!\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading the file: {e}\")\n",
        "    print(\"Please check your internet connection or the URL.\")\n",
        "\n",
        "# Let's quickly inspect the first few characters of the downloaded text\n",
        "with open(text_file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "    sample_text = f.read(500) # Read the first 500 characters\n",
        "print(\"\\n--- Sample of the downloaded text ---\")\n",
        "print(sample_text)\n",
        "print(\"------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rycy28898RN",
        "outputId": "987a2a8c-a843-41da-e24d-eeaf078fad7c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'alice_in_wonderland.txt' downloaded successfully!\n",
            "\n",
            "--- Sample of the downloaded text ---\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n",
            "\n",
            "[Illustration]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Alice’s Adventures in Wonderland\n",
            "\n",
            "by Lewis Carroll\n",
            "\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\n",
            "\n",
            "Contents\n",
            "\n",
            " CHAPTER I.     Down the Rabbit-Hole\n",
            " CHAPTER II.    The Pool of Tears\n",
            " CHAPTER III.   A Caucus-Race and a Long Tale\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
            " CHAPTER V.     Advice from a Caterpillar\n",
            " CHAPTER VI.    Pig and Pepper\n",
            " CHAPTER VII.   A Mad Tea-Party\n",
            " CHAPTER VIII.  The Queen’s Croquet-Ground\n",
            " CHAPTER IX.    The\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 2\n"
      ],
      "metadata": {
        "id": "PEv4NZbnXupO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # New import for tokenization\n",
        "from tensorflow.keras.utils import to_categorical # New import for one-hot encoding\n",
        "import re\n",
        "\n",
        "# 1. Load the text (same as before)\n",
        "text_file_name = \"alice_in_wonderland.txt\"\n",
        "with open(text_file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# --- 2. Text Cleaning for Word-Level (Modified) ---\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "# Remove numbers and most punctuation, keep letters and spaces\n",
        "# This is more aggressive to get clean words\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "# Replace multiple spaces with a single space\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "print(f\"Total length of cleaned text: {len(text)} characters\")\n",
        "\n",
        "# --- 3. Word Tokenization (New/Major Change) ---\n",
        "# Initialize Keras Tokenizer\n",
        "tokenizer = Tokenizer(oov_token='<unk>') # Handles out-of-vocabulary words\n",
        "tokenizer.fit_on_texts([text]) # Builds the vocabulary from your text\n",
        "\n",
        "# Convert text to sequence of word indices\n",
        "word_sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# Get word-to-index and index-to-word mappings\n",
        "word_to_int = tokenizer.word_index\n",
        "int_to_word = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "# Calculate vocabulary size (+1 because Keras tokenizer indices start from 1)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"Total unique words (vocabulary size): {vocab_size}\")\n",
        "\n",
        "print(\"\\n--- Sample Word Vocabulary (first 20) ---\")\n",
        "print(list(word_to_int.keys())[:20])\n",
        "print(\"-------------------------\\n\")\n",
        "\n",
        "# --- 4. Define Sequence Length (Adjusted for Words) ---\n",
        "seq_length = 50 # Now refers to 50 words, not characters\n",
        "\n",
        "# --- 5. Prepare Input and Target Sequences (Modified) ---\n",
        "dataX = [] # Input sequences of word indices\n",
        "dataY = [] # Target word index\n",
        "\n",
        "for i in range(0, len(word_sequences) - seq_length):\n",
        "    seq_in = word_sequences[i:i + seq_length]\n",
        "    seq_out = word_sequences[i + seq_length]\n",
        "    dataX.append(seq_in)\n",
        "    dataY.append(seq_out)\n",
        "\n",
        "n_patterns = len(dataX)\n",
        "print(f\"Total patterns (sequences) for training: {n_patterns}\")\n",
        "\n",
        "# --- 6. Reshape and One-Hot Encode Data (Modified) ---\n",
        "# X is now a simple NumPy array of word indices (samples, timesteps)\n",
        "X = np.array(dataX)\n",
        "\n",
        "# y is one-hot encoded, with num_classes as the word vocabulary size\n",
        "y = to_categorical(dataY, num_classes=vocab_size)\n",
        "\n",
        "print(f\"Shape of X (input sequences): {X.shape}\")\n",
        "print(f\"Shape of y (target words, one-hot encoded): {y.shape}\")\n",
        "print(f\"Vocabulary Size for One-Hot Encoding: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rx6IS_jo9-0Z",
        "outputId": "d5e771e5-b8c9-4515-b7aa-a2e6ba962777"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total length of cleaned text: 134617 characters\n",
            "Total unique words (vocabulary size): 2764\n",
            "\n",
            "--- Sample Word Vocabulary (first 20) ---\n",
            "['<unk>', 'the', 'and', 'to', 'a', 'she', 'it', 'of', 'said', 'i', 'alice', 'in', 'you', 'was', 'that', 'as', 'her', 'at', 'on', 'with']\n",
            "-------------------------\n",
            "\n",
            "Total patterns (sequences) for training: 26426\n",
            "Shape of X (input sequences): (26426, 50)\n",
            "Shape of y (target words, one-hot encoded): (26426, 2764)\n",
            "Vocabulary Size for One-Hot Encoding: 2764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the model for word-level generation\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding Layer:\n",
        "# input_dim: Size of the vocabulary (+1 because word indices start from 1)\n",
        "# output_dim: The dimensionality of the word embeddings (e.g., 256, 100, 50 - experiment with this!)\n",
        "# input_length: The length of input sequences (seq_length from preprocessing)\n",
        "embedding_dim = 256 # You can experiment with 100, 256, 512\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length))\n",
        "\n",
        "# LSTM Layers (similar to character-level, but now processing word embeddings)\n",
        "model.add(LSTM(256, return_sequences=True)) # First LSTM layer\n",
        "model.add(Dropout(0.2)) # Dropout to prevent overfitting\n",
        "model.add(LSTM(256)) # Second LSTM layer\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer: predicts probabilities for the next WORD\n",
        "model.add(Dense(vocab_size, activation='softmax')) # Output layer with softmax activation\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Print the model summary to see its structure\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "rUrlQiDd-ID6",
        "outputId": "3085a519-c6b9-422a-a2f5-3013245ad96a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the checkpoint callback (keep this as is)\n",
        "# This will save the model weights whenever there's an improvement in loss\n",
        "filepath=\"word-level-weights-improvement-{epoch:02d}-{loss:.4f}.keras\" # Renamed filename for clarity\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Train the model\n",
        "# You can adjust the number of epochs and batch_size\n",
        "print(\"Starting WORD-LEVEL model training. This will take significantly longer...\")\n",
        "# Ensure X and y are the word-level arrays from Step 3\n",
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list) # Retaining 100 epochs, but you could try more\n",
        "print(\"Word-Level Model training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaMjnid0-ame",
        "outputId": "847483f9-dfc1-466b-9ce2-3f25eb68d4b7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting WORD-LEVEL model training. This will take significantly longer...\n",
            "Epoch 1/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 6.5923\n",
            "Epoch 1: loss improved from inf to 6.29122, saving model to word-level-weights-improvement-01-6.2912.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - loss: 6.5909\n",
            "Epoch 2/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.9390\n",
            "Epoch 2: loss improved from 6.29122 to 5.91979, saving model to word-level-weights-improvement-02-5.9198.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 5.9387\n",
            "Epoch 3/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.6874\n",
            "Epoch 3: loss improved from 5.91979 to 5.71024, saving model to word-level-weights-improvement-03-5.7102.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 5.6878\n",
            "Epoch 4/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.5273\n",
            "Epoch 4: loss improved from 5.71024 to 5.54428, saving model to word-level-weights-improvement-04-5.5443.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 5.5276\n",
            "Epoch 5/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.3671\n",
            "Epoch 5: loss improved from 5.54428 to 5.37800, saving model to word-level-weights-improvement-05-5.3780.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 5.3673\n",
            "Epoch 6/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 5.1726\n",
            "Epoch 6: loss improved from 5.37800 to 5.21132, saving model to word-level-weights-improvement-06-5.2113.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 5.1731\n",
            "Epoch 7/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 5.0407\n",
            "Epoch 7: loss improved from 5.21132 to 5.05530, saving model to word-level-weights-improvement-07-5.0553.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 5.0409\n",
            "Epoch 8/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.8848\n",
            "Epoch 8: loss improved from 5.05530 to 4.91313, saving model to word-level-weights-improvement-08-4.9131.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 4.8853\n",
            "Epoch 9/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.7613\n",
            "Epoch 9: loss improved from 4.91313 to 4.78966, saving model to word-level-weights-improvement-09-4.7897.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 4.7618\n",
            "Epoch 10/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.6604\n",
            "Epoch 10: loss improved from 4.78966 to 4.68042, saving model to word-level-weights-improvement-10-4.6804.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 4.6607\n",
            "Epoch 11/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.5595\n",
            "Epoch 11: loss improved from 4.68042 to 4.57784, saving model to word-level-weights-improvement-11-4.5778.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 4.5597\n",
            "Epoch 12/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.4361\n",
            "Epoch 12: loss improved from 4.57784 to 4.48001, saving model to word-level-weights-improvement-12-4.4800.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 4.4367\n",
            "Epoch 13/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.3568\n",
            "Epoch 13: loss improved from 4.48001 to 4.38837, saving model to word-level-weights-improvement-13-4.3884.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 4.3572\n",
            "Epoch 14/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.2901\n",
            "Epoch 14: loss improved from 4.38837 to 4.29978, saving model to word-level-weights-improvement-14-4.2998.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 4.2902\n",
            "Epoch 15/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.1643\n",
            "Epoch 15: loss improved from 4.29978 to 4.21693, saving model to word-level-weights-improvement-15-4.2169.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 4.1651\n",
            "Epoch 16/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 4.0838\n",
            "Epoch 16: loss improved from 4.21693 to 4.13832, saving model to word-level-weights-improvement-16-4.1383.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 4.0844\n",
            "Epoch 17/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 4.0074\n",
            "Epoch 17: loss improved from 4.13832 to 4.06201, saving model to word-level-weights-improvement-17-4.0620.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 4.0082\n",
            "Epoch 18/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.9529\n",
            "Epoch 18: loss improved from 4.06201 to 3.98766, saving model to word-level-weights-improvement-18-3.9877.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 3.9534\n",
            "Epoch 19/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.8838\n",
            "Epoch 19: loss improved from 3.98766 to 3.91605, saving model to word-level-weights-improvement-19-3.9160.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 3.8843\n",
            "Epoch 20/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.7840\n",
            "Epoch 20: loss improved from 3.91605 to 3.84080, saving model to word-level-weights-improvement-20-3.8408.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 3.7848\n",
            "Epoch 21/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.7444\n",
            "Epoch 21: loss improved from 3.84080 to 3.77120, saving model to word-level-weights-improvement-21-3.7712.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 3.7446\n",
            "Epoch 22/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.6570\n",
            "Epoch 22: loss improved from 3.77120 to 3.69973, saving model to word-level-weights-improvement-22-3.6997.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 3.6576\n",
            "Epoch 23/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.6052\n",
            "Epoch 23: loss improved from 3.69973 to 3.64185, saving model to word-level-weights-improvement-23-3.6419.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 3.6057\n",
            "Epoch 24/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.5361\n",
            "Epoch 24: loss improved from 3.64185 to 3.57481, saving model to word-level-weights-improvement-24-3.5748.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 3.5365\n",
            "Epoch 25/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.4589\n",
            "Epoch 25: loss improved from 3.57481 to 3.50821, saving model to word-level-weights-improvement-25-3.5082.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 3.4596\n",
            "Epoch 26/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.4030\n",
            "Epoch 26: loss improved from 3.50821 to 3.44608, saving model to word-level-weights-improvement-26-3.4461.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 3.4034\n",
            "Epoch 27/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.3499\n",
            "Epoch 27: loss improved from 3.44608 to 3.39146, saving model to word-level-weights-improvement-27-3.3915.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 3.3505\n",
            "Epoch 28/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.2771\n",
            "Epoch 28: loss improved from 3.39146 to 3.33157, saving model to word-level-weights-improvement-28-3.3316.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 3.2779\n",
            "Epoch 29/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.2342\n",
            "Epoch 29: loss improved from 3.33157 to 3.27607, saving model to word-level-weights-improvement-29-3.2761.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 3.2346\n",
            "Epoch 30/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.1724\n",
            "Epoch 30: loss improved from 3.27607 to 3.21954, saving model to word-level-weights-improvement-30-3.2195.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 3.1731\n",
            "Epoch 31/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.1240\n",
            "Epoch 31: loss improved from 3.21954 to 3.16170, saving model to word-level-weights-improvement-31-3.1617.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 3.1241\n",
            "Epoch 32/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 3.0639\n",
            "Epoch 32: loss improved from 3.16170 to 3.11961, saving model to word-level-weights-improvement-32-3.1196.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 3.0647\n",
            "Epoch 33/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 3.0345\n",
            "Epoch 33: loss improved from 3.11961 to 3.06362, saving model to word-level-weights-improvement-33-3.0636.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 3.0349\n",
            "Epoch 34/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.9735\n",
            "Epoch 34: loss improved from 3.06362 to 3.01598, saving model to word-level-weights-improvement-34-3.0160.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 2.9739\n",
            "Epoch 35/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.9136\n",
            "Epoch 35: loss improved from 3.01598 to 2.96870, saving model to word-level-weights-improvement-35-2.9687.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 2.9144\n",
            "Epoch 36/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.8955\n",
            "Epoch 36: loss improved from 2.96870 to 2.93229, saving model to word-level-weights-improvement-36-2.9323.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 2.8959\n",
            "Epoch 37/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.8432\n",
            "Epoch 37: loss improved from 2.93229 to 2.87747, saving model to word-level-weights-improvement-37-2.8775.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 2.8437\n",
            "Epoch 38/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.7794\n",
            "Epoch 38: loss improved from 2.87747 to 2.83767, saving model to word-level-weights-improvement-38-2.8377.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 2.7803\n",
            "Epoch 39/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.7581\n",
            "Epoch 39: loss improved from 2.83767 to 2.79856, saving model to word-level-weights-improvement-39-2.7986.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 2.7585\n",
            "Epoch 40/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.6985\n",
            "Epoch 40: loss improved from 2.79856 to 2.75654, saving model to word-level-weights-improvement-40-2.7565.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 2.6993\n",
            "Epoch 41/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.6806\n",
            "Epoch 41: loss improved from 2.75654 to 2.71693, saving model to word-level-weights-improvement-41-2.7169.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 2.6811\n",
            "Epoch 42/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.6063\n",
            "Epoch 42: loss improved from 2.71693 to 2.66977, saving model to word-level-weights-improvement-42-2.6698.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 2.6066\n",
            "Epoch 43/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.5932\n",
            "Epoch 43: loss improved from 2.66977 to 2.63512, saving model to word-level-weights-improvement-43-2.6351.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 2.5938\n",
            "Epoch 44/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.5353\n",
            "Epoch 44: loss improved from 2.63512 to 2.59287, saving model to word-level-weights-improvement-44-2.5929.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 2.5359\n",
            "Epoch 45/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.5186\n",
            "Epoch 45: loss improved from 2.59287 to 2.55955, saving model to word-level-weights-improvement-45-2.5596.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 2.5192\n",
            "Epoch 46/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.4709\n",
            "Epoch 46: loss improved from 2.55955 to 2.51636, saving model to word-level-weights-improvement-46-2.5164.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 2.4716\n",
            "Epoch 47/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.4451\n",
            "Epoch 47: loss improved from 2.51636 to 2.48918, saving model to word-level-weights-improvement-47-2.4892.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 2.4455\n",
            "Epoch 48/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.4098\n",
            "Epoch 48: loss improved from 2.48918 to 2.44929, saving model to word-level-weights-improvement-48-2.4493.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 2.4104\n",
            "Epoch 49/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.3606\n",
            "Epoch 49: loss improved from 2.44929 to 2.41605, saving model to word-level-weights-improvement-49-2.4161.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 2.3612\n",
            "Epoch 50/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.3112\n",
            "Epoch 50: loss improved from 2.41605 to 2.37876, saving model to word-level-weights-improvement-50-2.3788.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 2.3122\n",
            "Epoch 51/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.2952\n",
            "Epoch 51: loss improved from 2.37876 to 2.34052, saving model to word-level-weights-improvement-51-2.3405.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 2.2956\n",
            "Epoch 52/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2817\n",
            "Epoch 52: loss improved from 2.34052 to 2.31417, saving model to word-level-weights-improvement-52-2.3142.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 2.2822\n",
            "Epoch 53/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.2484\n",
            "Epoch 53: loss improved from 2.31417 to 2.27826, saving model to word-level-weights-improvement-53-2.2783.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 2.2488\n",
            "Epoch 54/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.2230\n",
            "Epoch 54: loss improved from 2.27826 to 2.25164, saving model to word-level-weights-improvement-54-2.2516.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 2.2231\n",
            "Epoch 55/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1944\n",
            "Epoch 55: loss improved from 2.25164 to 2.22204, saving model to word-level-weights-improvement-55-2.2220.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 2.1945\n",
            "Epoch 56/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1397\n",
            "Epoch 56: loss improved from 2.22204 to 2.19043, saving model to word-level-weights-improvement-56-2.1904.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 2.1404\n",
            "Epoch 57/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.1275\n",
            "Epoch 57: loss improved from 2.19043 to 2.16276, saving model to word-level-weights-improvement-57-2.1628.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 2.1278\n",
            "Epoch 58/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 2.0947\n",
            "Epoch 58: loss improved from 2.16276 to 2.12987, saving model to word-level-weights-improvement-58-2.1299.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 2.0952\n",
            "Epoch 59/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 2.0672\n",
            "Epoch 59: loss improved from 2.12987 to 2.11222, saving model to word-level-weights-improvement-59-2.1122.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 2.0679\n",
            "Epoch 60/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 2.0195\n",
            "Epoch 60: loss improved from 2.11222 to 2.07760, saving model to word-level-weights-improvement-60-2.0776.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 2.0203\n",
            "Epoch 61/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.9995\n",
            "Epoch 61: loss improved from 2.07760 to 2.04470, saving model to word-level-weights-improvement-61-2.0447.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 2.0001\n",
            "Epoch 62/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.9879\n",
            "Epoch 62: loss improved from 2.04470 to 2.01674, saving model to word-level-weights-improvement-62-2.0167.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.9881\n",
            "Epoch 63/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.9415\n",
            "Epoch 63: loss improved from 2.01674 to 1.99145, saving model to word-level-weights-improvement-63-1.9914.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.9423\n",
            "Epoch 64/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.8987\n",
            "Epoch 64: loss improved from 1.99145 to 1.95363, saving model to word-level-weights-improvement-64-1.9536.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.8994\n",
            "Epoch 65/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.9018\n",
            "Epoch 65: loss improved from 1.95363 to 1.95111, saving model to word-level-weights-improvement-65-1.9511.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.9020\n",
            "Epoch 66/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.8681\n",
            "Epoch 66: loss improved from 1.95111 to 1.91373, saving model to word-level-weights-improvement-66-1.9137.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.8687\n",
            "Epoch 67/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.8608\n",
            "Epoch 67: loss improved from 1.91373 to 1.89960, saving model to word-level-weights-improvement-67-1.8996.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.8612\n",
            "Epoch 68/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.8188\n",
            "Epoch 68: loss improved from 1.89960 to 1.86624, saving model to word-level-weights-improvement-68-1.8662.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.8194\n",
            "Epoch 69/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.8001\n",
            "Epoch 69: loss improved from 1.86624 to 1.84200, saving model to word-level-weights-improvement-69-1.8420.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.8005\n",
            "Epoch 70/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7706\n",
            "Epoch 70: loss improved from 1.84200 to 1.80634, saving model to word-level-weights-improvement-70-1.8063.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.7708\n",
            "Epoch 71/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.7517\n",
            "Epoch 71: loss improved from 1.80634 to 1.78541, saving model to word-level-weights-improvement-71-1.7854.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.7522\n",
            "Epoch 72/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.7309\n",
            "Epoch 72: loss improved from 1.78541 to 1.76354, saving model to word-level-weights-improvement-72-1.7635.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.7310\n",
            "Epoch 73/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6897\n",
            "Epoch 73: loss improved from 1.76354 to 1.74384, saving model to word-level-weights-improvement-73-1.7438.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.6905\n",
            "Epoch 74/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6682\n",
            "Epoch 74: loss improved from 1.74384 to 1.71331, saving model to word-level-weights-improvement-74-1.7133.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.6689\n",
            "Epoch 75/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6465\n",
            "Epoch 75: loss improved from 1.71331 to 1.69423, saving model to word-level-weights-improvement-75-1.6942.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.6467\n",
            "Epoch 76/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.6420\n",
            "Epoch 76: loss improved from 1.69423 to 1.67124, saving model to word-level-weights-improvement-76-1.6712.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.6424\n",
            "Epoch 77/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6040\n",
            "Epoch 77: loss improved from 1.67124 to 1.64878, saving model to word-level-weights-improvement-77-1.6488.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.6044\n",
            "Epoch 78/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5882\n",
            "Epoch 78: loss improved from 1.64878 to 1.62981, saving model to word-level-weights-improvement-78-1.6298.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.5888\n",
            "Epoch 79/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5643\n",
            "Epoch 79: loss improved from 1.62981 to 1.59914, saving model to word-level-weights-improvement-79-1.5991.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.5645\n",
            "Epoch 80/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5566\n",
            "Epoch 80: loss improved from 1.59914 to 1.58103, saving model to word-level-weights-improvement-80-1.5810.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.5570\n",
            "Epoch 81/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.5236\n",
            "Epoch 81: loss improved from 1.58103 to 1.55135, saving model to word-level-weights-improvement-81-1.5513.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.5240\n",
            "Epoch 82/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4938\n",
            "Epoch 82: loss improved from 1.55135 to 1.53685, saving model to word-level-weights-improvement-82-1.5369.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.4944\n",
            "Epoch 83/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4713\n",
            "Epoch 83: loss improved from 1.53685 to 1.50461, saving model to word-level-weights-improvement-83-1.5046.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.4718\n",
            "Epoch 84/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4702\n",
            "Epoch 84: loss improved from 1.50461 to 1.49847, saving model to word-level-weights-improvement-84-1.4985.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.4706\n",
            "Epoch 85/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.4299\n",
            "Epoch 85: loss improved from 1.49847 to 1.47010, saving model to word-level-weights-improvement-85-1.4701.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.4305\n",
            "Epoch 86/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4202\n",
            "Epoch 86: loss improved from 1.47010 to 1.45380, saving model to word-level-weights-improvement-86-1.4538.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.4204\n",
            "Epoch 87/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.4120\n",
            "Epoch 87: loss improved from 1.45380 to 1.43996, saving model to word-level-weights-improvement-87-1.4400.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.4122\n",
            "Epoch 88/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.3718\n",
            "Epoch 88: loss improved from 1.43996 to 1.40416, saving model to word-level-weights-improvement-88-1.4042.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.3723\n",
            "Epoch 89/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3619\n",
            "Epoch 89: loss improved from 1.40416 to 1.38998, saving model to word-level-weights-improvement-89-1.3900.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.3623\n",
            "Epoch 90/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3356\n",
            "Epoch 90: loss improved from 1.38998 to 1.36770, saving model to word-level-weights-improvement-90-1.3677.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.3359\n",
            "Epoch 91/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3057\n",
            "Epoch 91: loss improved from 1.36770 to 1.35176, saving model to word-level-weights-improvement-91-1.3518.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.3064\n",
            "Epoch 92/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.3077\n",
            "Epoch 92: loss improved from 1.35176 to 1.32968, saving model to word-level-weights-improvement-92-1.3297.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.3080\n",
            "Epoch 93/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2904\n",
            "Epoch 93: loss improved from 1.32968 to 1.32334, saving model to word-level-weights-improvement-93-1.3233.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.2905\n",
            "Epoch 94/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2754\n",
            "Epoch 94: loss improved from 1.32334 to 1.30254, saving model to word-level-weights-improvement-94-1.3025.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.2755\n",
            "Epoch 95/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2419\n",
            "Epoch 95: loss improved from 1.30254 to 1.28050, saving model to word-level-weights-improvement-95-1.2805.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.2424\n",
            "Epoch 96/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.2348\n",
            "Epoch 96: loss improved from 1.28050 to 1.25233, saving model to word-level-weights-improvement-96-1.2523.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.2351\n",
            "Epoch 97/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1956\n",
            "Epoch 97: loss improved from 1.25233 to 1.23090, saving model to word-level-weights-improvement-97-1.2309.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 1.1961\n",
            "Epoch 98/100\n",
            "\u001b[1m206/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.2123\n",
            "Epoch 98: loss did not improve from 1.23090\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.2126\n",
            "Epoch 99/100\n",
            "\u001b[1m205/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1783\n",
            "Epoch 99: loss improved from 1.23090 to 1.20996, saving model to word-level-weights-improvement-99-1.2100.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.1788\n",
            "Epoch 100/100\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.1591\n",
            "Epoch 100: loss improved from 1.20996 to 1.19740, saving model to word-level-weights-improvement-100-1.1974.keras\n",
            "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - loss: 1.1593\n",
            "Word-Level Model training complete!\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# --- Ensure the following are already defined from previous steps ---\n",
        "# text, tokenizer, model, int_to_word, vocab_size, seq_length\n",
        "\n",
        "# Load the best weights\n",
        "try:\n",
        "    list_of_files = glob.glob('word-level-weights-improvement-*.keras')\n",
        "    latest_file = max(list_of_files, key=os.path.getctime) if list_of_files else None\n",
        "\n",
        "    if latest_file:\n",
        "        model.load_weights(latest_file)\n",
        "        print(f\"Loaded word-level model weights from: {latest_file}\")\n",
        "    else:\n",
        "        print(\"No word-level model weights found. Please ensure training completed and files were saved.\")\n",
        "        print(\"Attempting to generate text with potentially untrained model (will likely be random).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model weights: {e}\")\n",
        "\n",
        "# Compile the model again after loading weights\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Word-level text generator with safe multinomial sampling\n",
        "def generate_word_text(model, text_data, tokenizer_obj, int_to_word_map, vocab_size, seq_length, num_words_to_generate, diversity=1.0):\n",
        "    original_word_sequences = tokenizer_obj.texts_to_sequences([text_data])[0]\n",
        "\n",
        "    start_index = np.random.randint(0, len(original_word_sequences) - seq_length - 1)\n",
        "    pattern_indices = original_word_sequences[start_index:start_index + seq_length]\n",
        "    seed_words = [int_to_word_map.get(idx, \"<unk>\") for idx in pattern_indices]\n",
        "\n",
        "    print(f\"\\n--- Seed: \\\"{' '.join(seed_words)}\\\" ---\")\n",
        "\n",
        "    generated_words = seed_words.copy()\n",
        "\n",
        "    for _ in range(num_words_to_generate):\n",
        "        x = np.array([pattern_indices])  # Shape: (1, seq_length)\n",
        "        prediction = model.predict(x, verbose=0)[0]  # Shape: (vocab_size,)\n",
        "\n",
        "        # Temperature sampling\n",
        "        prediction = np.asarray(prediction).astype('float64')\n",
        "        prediction = np.log(prediction + 1e-10) / diversity\n",
        "        exp_preds = np.exp(prediction)\n",
        "        prediction = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "        # Precision-safe fix: Normalize & clip\n",
        "        prediction = np.maximum(prediction, 1e-12)\n",
        "        prediction = prediction / np.sum(prediction)\n",
        "        prediction[-1] = 1.0 - np.sum(prediction[:-1])  # Force sum to exactly 1.0\n",
        "\n",
        "        if prediction[-1] < 0:  # If adjustment broke the distribution\n",
        "            prediction = prediction / np.sum(prediction)\n",
        "\n",
        "        # Sample next word index\n",
        "        try:\n",
        "            probas = np.random.multinomial(1, prediction, 1)\n",
        "        except ValueError:\n",
        "            # As backup: use uniform distribution if probabilities are invalid\n",
        "            print(\"Warning: Probabilities invalid. Falling back to uniform sampling.\")\n",
        "            prediction = np.ones(vocab_size) / vocab_size\n",
        "            probas = np.random.multinomial(1, prediction, 1)\n",
        "\n",
        "        next_word_index = np.argmax(probas)\n",
        "        next_word = int_to_word_map.get(next_word_index, '<unk>')\n",
        "        generated_words.append(next_word)\n",
        "\n",
        "        pattern_indices = pattern_indices[1:] + [next_word_index]\n",
        "\n",
        "    return \" \".join(generated_words)\n",
        "\n",
        "# Generation parameters\n",
        "num_words_to_generate = 100\n",
        "diversity_values = [0.2, 0.5, 0.8]\n",
        "\n",
        "for diversity_val in diversity_values:\n",
        "    print(f\"\\n--- Generated Text (Diversity: {diversity_val}) ---\")\n",
        "    output = generate_word_text(\n",
        "        model, text, tokenizer, int_to_word, vocab_size,\n",
        "        seq_length, num_words_to_generate, diversity_val\n",
        "    )\n",
        "    print(output)\n",
        "    print(\"--------------------------------------------------\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gvbq84OBhy5",
        "outputId": "e71458df-8528-476f-dbeb-02c2e3110086"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded word-level model weights from: word-level-weights-improvement-100-1.1974.keras\n",
            "\n",
            "--- Generated Text (Diversity: 0.2) ---\n",
            "\n",
            "--- Seed: \"very easy to take more than nothing nobody asked your opinion said alice whos making personal remarks now the hatter asked triumphantly alice did not quite know what to say to this so she helped herself to some tea and breadandbutter and then turned to the dormouse and repeated her\" ---\n",
            "very easy to take more than nothing nobody asked your opinion said alice whos making personal remarks now the hatter asked triumphantly alice did not quite know what to say to this so she helped herself to some tea and breadandbutter and then turned to the dormouse and repeated her question why did they live at the bottom of a well take a table with the first day said the gryphon i mean what makes the matter worse you must have been changed for mabel ill try the dormouse say how her hurried back to the game the queen merely remarking that a moments delay would cost them their lives a mad teaparty he had not not left off the most confusing thing i ever heard yes please said alice in a tone of great surprise of course not said alice a little timidly but its no use going to\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Generated Text (Diversity: 0.5) ---\n",
            "\n",
            "--- Seed: \"a butterfly i should think youll feel it a little queer wont you not a bit said the caterpillar well perhaps your feelings may be different said alice all i know is it would feel very queer to me you said the caterpillar contemptuously who are you which brought them\" ---\n",
            "a butterfly i should think youll feel it a little queer wont you not a bit said the caterpillar well perhaps your feelings may be different said alice all i know is it would feel very queer to me you said the caterpillar contemptuously who are you which brought them back in a natural way i should like you very rude the mock turtle interrupted and thats as i hate cats and dogs it say as she went back to alice and she began to send the hedgehog to keep my business alice said to herself as she had got to the fifth bend i think and had going to say that said the cat i never heard to be afraid of this and she said to herself imagine the direction all my life she had to stoop to save her neck from your adventures i think what will become\n",
            "--------------------------------------------------\n",
            "\n",
            "--- Generated Text (Diversity: 0.8) ---\n",
            "\n",
            "--- Seed: \"the knave the knave shook his head sadly do i look like it he said which he certainly did not being made entirely of cardboard all right so far said the king and he went on muttering over the verses to himself we know it to be true thats the\" ---\n",
            "the knave the knave shook his head sadly do i look like it he said which he certainly did not being made entirely of cardboard all right so far said the king and he went on muttering over the verses to himself we know it to be true thats the same words out of the window and it was as much as it was no one of them and gave a little hot to look and she tried to come downhere sleepy and saying to herself well ive often had mad you dont like the look of the lobster quadrille said the gryphon as if a little sleep youve had like a frog and two went to the door and knocked theres no sort of use in all the case what do said alice hastily whatever persisted the king nothing whatever said alice and he got up and began to\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import json\n",
        "\n",
        "# Define paths for saving\n",
        "model_save_path = \"word_level_text_generator_model.keras\" # Saved after training in Step 5\n",
        "tokenizer_save_path = \"tokenizer.pkl\"\n",
        "int_to_word_map_path = \"int_to_word.json\"\n",
        "\n",
        "# Save the trained model (if you didn't do it via ModelCheckpoint)\n",
        "# You likely already have it saved from ModelCheckpoint, but this ensures a clean single file\n",
        "# model.save(model_save_path) # Uncomment if you want to explicitly save the final model\n",
        "\n",
        "# Save the tokenizer object\n",
        "with open(tokenizer_save_path, 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(f\"Tokenizer saved to {tokenizer_save_path}\")\n",
        "\n",
        "# Save the int_to_word map\n",
        "with open(int_to_word_map_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(int_to_word, f, ensure_ascii=False, indent=4)\n",
        "print(f\"int_to_word map saved to {int_to_word_map_path}\")\n",
        "\n",
        "print(\"All necessary assets saved for deployment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVhvXiUfgmQI",
        "outputId": "c129cda1-1ff7-458e-f8ec-7d796e26e923"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer saved to tokenizer.pkl\n",
            "int_to_word map saved to int_to_word.json\n",
            "All necessary assets saved for deployment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # Import here for loading tokenizer\n",
        "\n",
        "# --- Constants and File Paths ---\n",
        "MODEL_PATH = 'word-level-weights-improvement-100-1.1974.keras' # IMPORTANT: Update with your best model filename\n",
        "TOKENIZER_PATH = 'tokenizer.pkl'\n",
        "INT_TO_WORD_MAP_PATH = 'int_to_word.json'\n",
        "SEQ_LENGTH = 50 # Must match the seq_length used during training\n",
        "\n",
        "# --- Global Variables for Model and Mappings ---\n",
        "model = None\n",
        "tokenizer = None\n",
        "int_to_word = None\n",
        "vocab_size = 0 # Will be derived from tokenizer after loading\n",
        "\n",
        "# --- Load Model and Assets Function ---\n",
        "def load_assets():\n",
        "    global model, tokenizer, int_to_word, vocab_size\n",
        "\n",
        "    # Load the model\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(MODEL_PATH)\n",
        "        print(f\"Model loaded from {MODEL_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        # Try loading weights if the full model save failed or if it's just weights file\n",
        "        print(f\"Attempting to load weights directly into a new model structure.\")\n",
        "        from tensorflow.keras.models import Sequential\n",
        "        from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "        # First, we need to load tokenizer to get vocab_size\n",
        "        with open(TOKENIZER_PATH, 'rb') as f:\n",
        "            tokenizer = pickle.load(f)\n",
        "        vocab_size = len(tokenizer.word_index) + 1\n",
        "        embedding_dim = 256 # Must match the embedding_dim used during training\n",
        "\n",
        "        temp_model = Sequential()\n",
        "        temp_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=SEQ_LENGTH))\n",
        "        temp_model.add(LSTM(256, return_sequences=True))\n",
        "        temp_model.add(Dropout(0.2))\n",
        "        temp_model.add(LSTM(256))\n",
        "        temp_model.add(Dropout(0.2))\n",
        "        temp_model.add(Dense(vocab_size, activation='softmax'))\n",
        "        temp_model.compile(loss='categorical_crossentropy', optimizer='adam') # Compile before loading weights\n",
        "\n",
        "        # IMPORTANT: Find the actual best weights file name\n",
        "        list_of_files = glob.glob('word-level-weights-improvement-*.keras')\n",
        "        latest_weights_file = max(list_of_files, key=os.path.getctime) if list_of_files else MODEL_PATH\n",
        "        if os.path.exists(latest_weights_file):\n",
        "            temp_model.load_weights(latest_weights_file)\n",
        "            print(f\"Loaded weights from {latest_weights_file}\")\n",
        "            model = temp_model\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Neither {MODEL_PATH} nor {latest_weights_file} found.\")\n",
        "\n",
        "\n",
        "    # Load the tokenizer\n",
        "    with open(TOKENIZER_PATH, 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    print(f\"Tokenizer loaded from {TOKENIZER_PATH}\")\n",
        "\n",
        "    # Load the int_to_word map\n",
        "    with open(INT_TO_WORD_MAP_PATH, 'r', encoding='utf-8') as f:\n",
        "        int_to_word = json.load(f)\n",
        "    print(f\"int_to_word map loaded from {INT_TO_WORD_MAP_PATH}\")\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1 # +1 for out-of-vocabulary token or padding\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# --- Text Generation Function (adapted for Gradio) ---\n",
        "def generate_text_for_gradio(seed_text, num_words_to_generate, diversity):\n",
        "    if model is None:\n",
        "        return \"Model not loaded. Please wait or check logs.\"\n",
        "\n",
        "    # Preprocess seed text\n",
        "    seed_text = seed_text.lower()\n",
        "    seed_sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\n",
        "    # Ensure seed_sequence is at least seq_length\n",
        "    if len(seed_sequence) < SEQ_LENGTH:\n",
        "        # Pad or extend with start tokens if necessary\n",
        "        # For simplicity, we'll just return an error or pad with known token if possible\n",
        "        # A more robust solution might pre-pend common words or padding\n",
        "        return f\"Seed text must be at least {SEQ_LENGTH} words long to generate meaningful text. Please try a longer seed.\"\n",
        "\n",
        "    # Trim seed sequence if too long\n",
        "    pattern_indices = seed_sequence[-SEQ_LENGTH:]\n",
        "\n",
        "    # Convert seed pattern (indices) back to words for display\n",
        "    seed_words = [int_to_word.get(idx, '<unk>') for idx in pattern_indices]\n",
        "    generated_words = []\n",
        "    generated_words.extend(seed_words)\n",
        "\n",
        "    for _ in range(num_words_to_generate):\n",
        "        x = np.array([pattern_indices])\n",
        "        prediction = model.predict(x, verbose=0)[0]\n",
        "\n",
        "        prediction = np.log(prediction + 1e-10) / diversity\n",
        "        exp_preds = np.exp(prediction)\n",
        "        prediction = exp_preds / np.sum(exp_preds)\n",
        "        prediction = np.maximum(prediction, 0)\n",
        "        prediction = prediction / np.sum(prediction)\n",
        "\n",
        "        if np.sum(prediction) == 0:\n",
        "            next_word_index = np.random.randint(0, vocab_size) # Fallback to random\n",
        "        else:\n",
        "            probas = np.random.multinomial(1, prediction, 1)\n",
        "            next_word_index = np.argmax(probas)\n",
        "\n",
        "        next_word = int_to_word.get(next_word_index, '<unk>')\n",
        "        generated_words.append(next_word)\n",
        "        pattern_indices = pattern_indices[1:] + [next_word_index]\n",
        "\n",
        "    return \" \".join(generated_words)\n",
        "\n",
        "# --- Gradio Interface ---\n",
        "# Load assets before launching the interface\n",
        "load_assets()\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=generate_text_for_gradio,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=f\"Enter a seed text (at least {SEQ_LENGTH} words) here...\", label=\"Seed Text\"),\n",
        "        gr.Slider(minimum=10, maximum=500, step=10, default=100, label=\"Number of words to generate\"),\n",
        "        gr.Slider(minimum=0.1, maximum=1.5, step=0.1, default=0.5, label=\"Diversity (Creativity)\", info=\"Lower values are more conservative, higher values are more creative/random.\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Alice in Wonderland Word-Level Text Generator\",\n",
        "    description=\"Generate text in the style of 'Alice's Adventures in Wonderland' using an LSTM neural network. Provide a seed text, and the model will continue the story word by word.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(debug=True) # debug=True is useful for local testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrVzEqKPgscb",
        "outputId": "ff863f3a-f071-4c1b-bf65-ddddabb451d3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "tensorflow\n",
        "numpy\n",
        "gradio\n",
        "scipy\n",
        "keras # Although part of tensorflow, sometimes good to explicitly list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUjkqHRDgxAM",
        "outputId": "783daee3-b345-41d2-8ad3-6e2a25d3375c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    }
  ]
}